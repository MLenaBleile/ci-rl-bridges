{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12420116,"sourceType":"datasetVersion","datasetId":7833568}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom tqdm import trange\n\n# global vars\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED) \n\n# Environment class\nclass GridWorldEnv:\n    def __init__(self, relative_ignorability=True, grid_size=2):\n        self.relative_ignorability = relative_ignorability\n        self.actions= ['right', 'down', 'left', 'up']\n        self.action_to_delta = {'right': (0, 1), 'down': (1, 0), 'left': (0, -1), 'up': (-1, 0)}\n        self.action_space = len(self.actions)\n        self.grid_size=grid_size\n        self.obs_space = 2  # x, y position\n\n    def reset(self):\n        self.pos = (0, 0)\n        self.mode = np.random.choice([0, 1])  # Relatively ignorable hidden variable\n        \n        return np.array(self.pos)\n\n    def step(self, action_idx):\n        action = self.actions[action_idx]\n        dx, dy = self.action_to_delta[action]\n\n        \n\n        new_x = np.clip(self.pos[0] + dx, 0, self.grid_size - 1)\n        new_y = np.clip(self.pos[1] + dy, 0, self.grid_size - 1)\n        self.pos = (new_x, new_y)\n\n        reward = -0.1\n        done = False\n        if self.pos == (0, grid_size):  # Goal\n            reward = (10 if self.mode == 0 else 8) if self.relative_ignorability else (-10 if self.mode == 0 else 10)\n            #reward = np.random.normal(loc=reward_mean, scale=1, size=1)\n            done = True\n        elif self.pos == (grid_size, grid_size):  # Trap\n            reward = -10 if self.relative_ignorability else (10 if self.mode == 0 else -10)\n            #reward = np.random.normal(loc=reward_mean, scale=1, size=1)\n            done = True\n\n        return np.array(self.pos), reward, done\n\n# Simple DQN\nclass DQN(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(DQN, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.Linear(64,output_size)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n# Training loop\ndef train_dqn(env, episodes=2000, seed=42, gamma=0.95,batch_size=64, epsilon_min = 0.2, epsilon_decay=0.99995, lr=0.0001):\n    model = DQN(input_size=env.obs_space, output_size=env.action_space)\n    target_model = DQN(input_size=env.obs_space, output_size=env.action_space)\n    target_model.load_state_dict(model.state_dict())\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    memory = deque(maxlen=10000)\n    #gamma = gamma\n    #batch_size = 64\n    epsilon = 1.0\n    #epsilon_min = 0.2\n    #epsilon_decay = 0.99995\n\n    rewards = []\n\n    for ep in range(episodes):\n        state = torch.tensor(env.reset(), dtype=torch.float32)\n        total_reward = 0\n        done = False\n\n        while not done:\n            if random.random() < epsilon:\n                action = np.random.randint(env.action_space)\n            else:\n                with torch.no_grad():\n                    q_vals = model(state.unsqueeze(0))\n                action = torch.argmax(q_vals).item()\n\n            next_state_np, reward, done = env.step(action)\n            next_state = torch.tensor(next_state_np, dtype=torch.float32)\n            memory.append((state, action, reward, next_state, done))\n            state = next_state\n            total_reward += reward\n\n            if len(memory) >= batch_size:\n                batch = random.sample(memory, batch_size)\n                states, actions, rewards_batch, next_states, dones = zip(*batch)\n\n                states = torch.stack(states)\n                actions = torch.tensor(actions)\n                rewards_batch = torch.tensor(rewards_batch)\n                next_states = torch.stack(next_states)\n                dones = torch.tensor(dones, dtype=torch.float32)\n\n                q_vals = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n                with torch.no_grad():\n                    max_next_q = target_model(next_states).max(dim=1)[0]\n                targets = rewards_batch + gamma * max_next_q * (1 - dones)\n\n                loss = F.mse_loss(q_vals, targets)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        if ep % 10 == 0:\n            target_model.load_state_dict(model.state_dict())\n\n        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n        rewards.append(total_reward)\n\n    return rewards\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T00:52:19.330959Z","iopub.execute_input":"2025-10-13T00:52:19.331338Z","iopub.status.idle":"2025-10-13T00:52:23.771904Z","shell.execute_reply.started":"2025-10-13T00:52:19.331306Z","shell.execute_reply":"2025-10-13T00:52:23.770966Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n\n# POMDP GridWorld\nclass GridWorldPOMDP:\n    def __init__(self, relative_ignorability=True, grid_size=2):\n        self.relative_ignorability = relative_ignorability\n        self.grid_size=grid_size\n        self.actions= ['right', 'down', 'left', 'up']\n        self.action_to_delta = {'right': (0, 1), 'down': (1, 0), 'left': (0, -1), 'up': (-1, 0)}\n        self.action_space = len(self.actions)\n        self.grid_size=grid_size\n        self.obs_space = 3 #x, y, belief (latent \"mode\")\n\n    def reset(self):\n        self.pos = (0, 0)\n        self.mode = np.random.choice([0, 1])  # hidden variable\n        self.observed_variable  = np.random.normal(loc=self.mode)\n        self.belief_mode = 0.5  # initial belief over hidden variable\n        return np.array([*self.pos, self.belief_mode], dtype=np.float32)\n\n    def step(self, action_idx):\n        action = self.actions[action_idx]\n        dx, dy = self.actions_to_delta[action]\n        new_x = np.clip(self.pos[0] + dx, 0, self.grid_size - 1)\n        new_y = np.clip(self.pos[1] + dy, 0, self.grid_size - 1)\n        self.pos = (new_x, new_y)\n        \n        reward = -0.1\n        done = False\n        if self.pos == (0, self.grid_size):  # Goal\n            reward = (10 if self.mode == 0 else 9) if self.relative_ignorability else (-10 if self.mode == 0 else 10)\n            done = True\n        elif self.pos == (self.grid_size, self.grid_size):  # Trap\n            reward = -10 if self.relative_ignorability else (10 if self.mode == 0 else -10)\n            done = True\n\n        # observe new evidence\n        self.observed_variable = np.random.normal(loc=self.mode, scale=0.15)\n    \n        # update belief based on new observation\n        self.update_belief(obs=self.observed_variable)\n    \n        return np.array([*self.pos, self.belief_mode], dtype=np.float32), reward, done\n\n    def update_belief(self, obs):\n        p1 = np.exp(-(obs - 1)**2 / 2)\n        p0 = np.exp(-(obs - 0)**2 / 2)\n        prior = self.belief_mode\n        self.belief_mode = (p1 * prior) / (p1 * prior + p0 * (1 - prior) + 1e-8)\n        \n\n# Neural Net with belief input\nclass BeliefDQN(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 64), \n            nn.ReLU(),\n            nn.Linear(64, output_size)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n# Training function\ndef train_pomdp_agent(env, episodes=2000, batch_size=64,seed=42, gamma=0.95, epsilon_decay=0.99995, epsilon_min=0.2, lr=0.0001):\n    \n    model = BeliefDQN(input_size=env.obs_space, output_size=env.action_space)\n    target_model = BeliefDQN()\n    target_model.load_state_dict(model.state_dict())\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    memory = deque(maxlen=10000)\n\n    epsilon = 1.0\n    \n    rewards = []\n\n    for ep in trange(episodes):\n        state = torch.tensor(env.reset(), dtype=torch.float32)\n        total_reward = 0\n        done = False\n\n        while not done:\n            if random.random() < epsilon:\n                action = np.random.randint(env.action_space)\n            else:\n                with torch.no_grad():\n                    q_vals = model(state.unsqueeze(0))\n                action = torch.argmax(q_vals).item()\n\n            next_obs, reward, done = env.step(action)\n            \n            next_obs[2] = env.belief_mode  # update belief component\n            next_state = torch.tensor(next_obs, dtype=torch.float32)\n\n            memory.append((state, action, reward, next_state, done))\n            state = next_state\n            total_reward += reward\n\n            if len(memory) >= batch_size:\n                batch = random.sample(memory, batch_size)\n                states, actions, rewards_batch, next_states, dones = zip(*batch)\n\n                states = torch.stack(states)\n                actions = torch.tensor(actions)\n                rewards_batch = torch.tensor(rewards_batch)\n                next_states = torch.stack(next_states)\n                dones = torch.tensor(dones, dtype=torch.float32)\n\n                q_vals = model(states).gather(1, actions.unsqueeze(1)).squeeze()\n                with torch.no_grad():\n                    max_next_q = target_model(next_states).max(1)[0]\n                targets = rewards_batch + gamma * max_next_q * (1 - dones)\n\n                loss = F.mse_loss(q_vals, targets)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n        if ep % 10 == 0:\n            target_model.load_state_dict(model.state_dict())\n\n        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n        rewards.append(total_reward)\n\n    return rewards\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T00:52:23.773318Z","iopub.execute_input":"2025-10-13T00:52:23.773791Z","iopub.status.idle":"2025-10-13T00:52:23.794697Z","shell.execute_reply.started":"2025-10-13T00:52:23.773746Z","shell.execute_reply":"2025-10-13T00:52:23.793604Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"M = 3\nepisodes = 1000\nseed_range = range(M*350)\nseeds = [random.randint(0,7392479) for _ in range(M)]\nm=0\ngamma = 0.9\nbatch_size = 32\nepsilon = 1.0\nepsilon_min = 0.05\nepsilon_decay = 0.995\nlr=0.001\ngamma=0.9\ngrid_size = 5\n\nresultsarray = np.empty((M, episodes, 2, 2))\nfor m in range(m,M):\n    print(m)\n    rewards_ri = train_dqn(GridWorldEnv(relative_ignorability=True), episodes=episodes, seed=seeds[m], lr=lr, epsilon_min=epsilon_min,gamma=gamma, epsilon_decay=epsilon_decay)\n    \n    rewards_non_ri = train_dqn(GridWorldEnv(relative_ignorability=False), episodes=episodes, seed=seeds[m], lr=lr, epsilon_min=epsilon_min,gamma=gamma, epsilon_decay=epsilon_decay)\n    \n    \n    rewards_pomdp_ri = train_pomdp_agent(GridWorldPOMDP(relative_ignorability=True), episodes= episodes, seed=seeds[m], lr=lr, epsilon_min=epsilon_min,gamma=gamma, epsilon_decay=epsilon_decay)\n    rewards_pomdp_nonri = train_pomdp_agent(GridWorldPOMDP(relative_ignorability=False), episodes=episodes, seed=seeds[m], lr=lr, epsilon_min=epsilon_min,gamma=gamma, epsilon_decay=epsilon_decay)\n\n    resultsarray[m,:,0,0] = rewards_ri\n    resultsarray[m,:,0,1] = rewards_non_ri\n    resultsarray[m,:,1,0] = rewards_pomdp_ri\n    resultsarray[m,:,1,1] = rewards_pomdp_nonri","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T00:52:23.795682Z","iopub.execute_input":"2025-10-13T00:52:23.796040Z","execution_failed":"2025-10-13T14:46:14.097Z"}},"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nresultsavg = np.nanmean(resultsarray, axis=0)\n# Compute rolling average\nwindow = 50\nsmoothed = np.empty_like(resultsavg)\nfor i in range(resultsavg.shape[1]):  # RI vs Non-RI\n    for j in range(resultsavg.shape[2]):  # Vanilla vs POMDP\n        smoothed[:, i, j] = pd.Series(resultsavg[:, i, j]).rolling(window=window, min_periods=1).mean()\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(smoothed[10:, 0, 0], label=\"RI Mode + Vanilla Q-learning\", color=\"green\")\nplt.plot(smoothed[10:, 0, 1], label=\"Non-RI Mode + Vanilla Q-learning\", color=\"green\", linestyle=\"--\")\nplt.plot(smoothed[10:, 1, 0], label=\"RI Mode + POMDP Q-learning\", color = \"black\")\nplt.plot(smoothed[10:, 1, 1], label=\"Non-RI Mode + POMDP Q-learning\", color=\"black\",linestyle=\"--\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Smoothed Total Reward\")\nplt.title(\"Smoothed Deep Q-Learning Performance\\nUnder Relative Ignorability vs. Non-Ignorability\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n#plt.show()\nplt.savefig(\"reward_curves.png\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T14:46:14.106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('resultsarray.npy', resultsarray)\n\n#resultsarray = np.load(\"/kaggle/input/relative-ignorability-simulation-results/resultsarray.npy\")\n#resultsavg[1000:1005,0,0]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-13T14:46:14.110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}