{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-14T17:18:47.520564Z\",\"iopub.execute_input\":\"2025-05-14T17:18:47.520861Z\",\"iopub.status.idle\":\"2025-05-14T17:20:23.808005Z\",\"shell.execute_reply.started\":\"2025-05-14T17:18:47.520836Z\",\"shell.execute_reply\":\"2025-05-14T17:20:23.806598Z\"}}\n!pip install numpy pandas torch arch\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-14T17:20:34.862380Z\",\"iopub.execute_input\":\"2025-05-14T17:20:34.862716Z\",\"iopub.status.idle\":\"2025-05-14T17:20:34.884504Z\",\"shell.execute_reply.started\":\"2025-05-14T17:20:34.862694Z\",\"shell.execute_reply\":\"2025-05-14T17:20:34.883517Z\"}}\n\n\"\"\"dyna_q_garch_model.py\n--------------------------------\nDyna‑Q example for a volatility‑forecasting task.\n\n* Environment: latent GARCH(1,1) generating log‑returns.\n* Agent: tabular Dyna‑Q.  Action = scale factor lambda applied to a baseline\n  GARCH forecast; this corresponds to the set price of the \"investor insurance\".  Planning uses a *fitted* GARCH(1,1) as the generative\n  model, so imagined roll‑outs can explore unseen volatility regimes.\n* Baseline: single GARCH(1,1) forecaster (lambda = 1).\n\nRequires: numpy, pandas, arch, tqdm\n\"\"\"\n\nimport numpy as np\nfrom collections import deque\nfrom arch import arch_model\nfrom tqdm import trange\n\n\n# ---------------------------------------------------------------------\n# 1.  Latent data‑generating process\n# ---------------------------------------------------------------------\ndef simulate_garch(n, omega=0.05, alpha=0.05, beta=0.9, seed=None):\n    #Simulate returns r_j and conditional variances sigma2_j.\n    if seed is not None:\n        np.random.seed(seed)\n    eps = np.random.normal(size=n)\n    sigma2 = np.empty(n)\n    r = np.empty(n)\n\n    sigma2[0] = omega / (1.0 - alpha - beta)          # unconditional var\n    r[0] = eps[0] * np.sqrt(sigma2[0])\n\n    for j in range(1, n):\n        sigma2[j] = omega + alpha * r[j - 1] ** 2 + beta * sigma2[j - 1]\n        r[j] = eps[j] * np.sqrt(sigma2[j])\n    return r, sigma2\n\n\n# ---------------------------------------------------------------------\n# 2.  Helper to fit GARCH(1,1) for baseline forecast\n# ---------------------------------------------------------------------\ndef fit_garch(returns):\n\n    #Fit a GARCH(1,1) and return the 1-step-ahead variance forecast.\n    \n\n    # For extremely short series fall back to the sample variance\n    if len(returns) < 5:\n        return float(np.var(returns, ddof=1)) if len(returns) > 1 else 0.0\n\n    am  = arch_model(returns, vol=\"GARCH\", p=1, q=1, dist=\"normal\")\n    res = am.fit(disp=\"off\")\n\n    fvar = res.forecast(horizon=1, reindex=False).variance.values\n    # Possible shapes: scalar (), 1-D (T,), 2-D (T,1)\n    if fvar.ndim == 0:          # scalar\n        return float(fvar)\n    elif fvar.ndim == 1:        # (T,)\n        return float(fvar[-1])\n    else:                       # (T,1)\n        return float(fvar[-1, 0])\n\n\n\n# ---------------------------------------------------------------------\n# 3.  Dyna‑Q agent whose planning model is a *fitted* GARCH(1,1)\n# ---------------------------------------------------------------------\nclass DynaQGarchAgent:\n    def __init__(self, lambdas, n_state_bins=20, gamma=0.95, alpha=0.3,\n                 eps=0.1, n_plan=10, refit_every=50):\n        self.lambdas = np.asarray(lambdas)\n        self.nA = len(lambdas)\n        self.nS = n_state_bins\n\n        self.gamma = gamma          # discount\n        self.alpha = alpha          # TD learning‑rate\n        self.eps = eps              # eps‑greedy exploration\n        self.n_plan = n_plan        # synthetic updates / real step\n        self.refit_every = refit_every\n\n        self.Q = np.zeros((self.nS, self.nA))\n        self.t_real = 0             # real environment step counter\n\n        # Parameters of the *planning* GARCH model (initial guess)\n        self.omega = 0.03\n        self.alpha_g = 0.07\n        self.beta_g = 0.7\n\n    # ----- state index ------------------------------------------------\n    def state_idx(self, sigma2, s_min, s_max):\n        return int(np.clip((sigma2 - s_min) / (s_max - s_min) * (self.nS - 1),\n                           0, self.nS - 1))\n\n    # ----- action selection ---------------------------------------------\n    def act(self, s):\n        if np.random.rand() < self.eps:\n            return np.random.randint(self.nA)\n        return int(np.argmax(self.Q[s]))\n\n    # ----- TD learning ---------------------------------------------------\n    def learn(self, s, a, r, sp):\n        td_err = r + self.gamma * np.max(self.Q[sp]) - self.Q[s, a]\n        self.Q[s, a] += self.alpha * td_err\n\n    # ------------------------------------------------------------------\n    # ============   P L A N N I N G   ( Dyna with GARCH) ==============\n    # ------------------------------------------------------------------\n    def planning_step(self, s_min, s_max):\n        for _ in range(self.n_plan):\n            # 1. sample hypothetical state uniformly across bins\n            x = np.random.randint(self.nS)\n            # convert bin centre back to σ_j²\n            sigma2_j = s_min + (x + 0.5) * (s_max - s_min) / self.nS\n\n            # 2. draw eps_j ~ N(0,1)\n            eps_j = np.random.normal()\n\n            # 3. one‑step GARCH(1,1) dynamics\n            sigma2_jp1 = (self.omega +\n                          self.alpha_g * (eps_j ** 2 * sigma2_j) +\n                          self.beta_g * sigma2_j)\n\n            # 4. successor state index\n            xp = self.state_idx(sigma2_jp1, s_min, s_max)\n\n            # 5. evaluate all actions from state s\n            for a, lam in enumerate(self.lambdas):\n                sigma2_hat = lam * sigma2_jp1\n                r = sigma2_hat - sigma2_jp1      # var‑swap P&L\n                self.learn(x, a, r, sp)\n\n    # ----- refit GARCH model on latest returns --------------------------\n    def refit_model(self, returns):\n        am = arch_model(returns, vol=\"GARCH\", p=1, q=1, dist=\"normal\")\n        res = am.fit(disp=\"off\")\n        self.omega = res.params[\"omega\"]\n        self.alpha_g = res.params[\"alpha[1]\"]\n        self.beta_g = res.params[\"beta[1]\"]\n\n# ---------------------------------------------------------------------\n# 4.  Training loop\n# ---------------------------------------------------------------------\ndef run_episode(n_steps=4000, burn_in=500, agent=None,\n                omega=0.05, alpha=0.05, beta=0.9, seed=None):\n    \"\"\"Run a single trajectory and return mean P&L for RL vs baseline.\"\"\"\n    returns, sigma2_true = simulate_garch(n_steps + 1, omega, alpha, beta, seed=seed)\n    s_min, s_max = np.percentile(sigma2_true[:burn_in], [2, 98])\n\n    hist_returns = []                      # data for online refits\n    cum_pnl_rl = cum_pnl_base = 0.0\n\n    for j in trange(burn_in, n_steps, leave=False, desc=\"training\", ncols=80):\n        # collect newest realised return\n        hist_returns.append(returns[j])\n\n        # 1. baseline single‑GARCH forecast\n        sig2_hat_base = fit_garch(np.array(hist_returns))\n\n        # 2. agent interaction\n        x = agent.state_idx(sigma2_true[j], s_min, s_max)\n        a = agent.act(x)\n        lam = agent.lambdas[a]\n        sig2_hat_rl = lam * sig2_hat_base\n\n        # realised reward (var‑swap P&L)\n        r_base = sig2_hat_base - sigma2_true[j + 1]\n        r_rl = sig2_hat_rl - sigma2_true[j + 1]\n\n        cum_pnl_base += r_base\n        cum_pnl_rl += r_rl\n\n        # real TD update\n        sp = agent.state_idx(sigma2_true[j + 1], s_min, s_max)\n        agent.learn(x, a, r_rl, sp)\n\n        # 3. refit internal GARCH & planning\n        agent.t_real += 1\n        if agent.t_real % agent.refit_every == 0:\n            agent.refit_model(np.array(hist_returns))\n        agent.planning_step(s_min, s_max)\n\n    n_eval = n_steps - burn_in\n    return cum_pnl_rl / n_eval, cum_pnl_base / n_eval\n\n\n\n\n\n# %% [code]\n# ---------------------------------------------------------------------\n# 5.  Script entry‑point\n# ---------------------------------------------------------------------\nif __name__ == \"__main__\":\n    np.random.seed(0)\n\n    lambdas = np.linspace(0.5, 1.5, 11)    # 0.50 … 1.50 in 0.05 steps\n    agent = DynaQGarchAgent(lambdas,\n                            n_state_bins=20,\n                            gamma=0.95,\n                            alpha=0.3,\n                            eps=0.1,\n                            n_plan=10,\n                            refit_every=50)\n\n    rl_pnl, base_pnl = run_episode(agent=agent,\n                                   n_steps=4000,\n                                   burn_in=500,\n                                   seed=1)\n\n    print(\"\\nResults\\n-------\")\n    print(f\"RL mean P&L:    {rl_pnl: .6f}\")\n    print(f\"Single‑GARCH:   {base_pnl: .6f}\")\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-14T17:23:06.832511Z\",\"iopub.execute_input\":\"2025-05-14T17:23:06.833218Z\",\"iopub.status.idle\":\"2025-05-14T17:35:55.105714Z\",\"shell.execute_reply.started\":\"2025-05-14T17:23:06.833193Z\",\"shell.execute_reply\":\"2025-05-14T17:35:55.104548Z\"}}\nif __name__ == \"__main__\":\n    num_trials =30\n    results = np.zeros((num_trials, 2), dtype=float)\n    for oneseed in range(num_trials):\n        np.random.seed(oneseed)\n        lambdas = np.linspace(0.5, 1.5, 11)    # 0.50 … 1.50 in 0.05 steps\n        agent = DynaQGarchAgent(lambdas,\n                                n_state_bins=20,\n                                gamma=0.95,\n                                alpha=0.3,\n                                eps=0.1,\n                                n_plan=10,\n                                refit_every=50)\n    \n        rl_pnl, base_pnl = run_episode(agent=agent,\n                                       n_steps=4000,\n                                       burn_in=500,\n                                       seed=oneseed)\n\n        results[oneseed,:] = [rl_pnl,base_pnl]\n        \n\n    print(\"\\nResults\\n-------\")\n    print(f\"RL mean P&L:    {rl_pnl: .6f}\")\n    print(f\"Single‑GARCH:   {base_pnl: .6f}\")\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-05-14T17:42:00.085175Z\",\"iopub.execute_input\":\"2025-05-14T17:42:00.085945Z\",\"iopub.status.idle\":\"2025-05-14T17:42:00.259175Z\",\"shell.execute_reply.started\":\"2025-05-14T17:42:00.085866Z\",\"shell.execute_reply\":\"2025-05-14T17:42:00.258296Z\"}}\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.DataFrame(results, columns=['RL-GARCH','GARCH'])\n# Create a boxplot\nplt.boxplot(df, vert=True, patch_artist=True, labels=['RL-GARCH','GARCH'])\n\n# Add title and labels\nplt.title('Results Across Monte Carlo Random Seeds')\nplt.xlabel('Method')\nplt.ylabel('Swap Returns')","metadata":{"_uuid":"9e34b534-209e-4499-880d-70b0800b8667","_cell_guid":"9b58f519-64f3-460c-b92d-fa15a5565d1d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}